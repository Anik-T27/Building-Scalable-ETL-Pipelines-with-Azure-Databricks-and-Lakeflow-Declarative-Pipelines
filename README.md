# Building-Scalable-ETL-Pipelines-with-Azure-Databricks-and-Lakeflow-Declarative-Pipelines

## ğŸ“Œ Overview  
This project implements an **end-to-end data engineering pipeline** for aviation datasets (airports, flights, passengers, bookings) using the **Databricks Lakehouse Platform**.  
It follows the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** with **Autoloader, Delta Live Tables (DLT), PySpark, SQL**, and supports **incremental loads, schema evolution, CDC, and SCD**.  
Deployed on **Databricks Serverless Compute**, it delivers **analytics-ready models** enabling insights into bookings, flights, passengers, and airport performance.  

---

## ğŸ—‚ï¸ Repository Structure 
aviation-data-engineering/
â”‚
â”œâ”€â”€ Datasets/ # Raw aviation datasets (CSV files)
â”‚ â”œâ”€â”€ airports.csv
â”‚ â”œâ”€â”€ flights.csv
â”‚ â”œâ”€â”€ passengers.csv
â”‚ â””â”€â”€ bookings.csv
â”‚
â”œâ”€â”€ Docx/ # Project documentation and diagrams
â”‚ â””â”€â”€ Architecture.png
â”‚
â”œâ”€â”€ Scripts/ # Python scripts for ingestion, DLT, and transformations
â”‚ â”œâ”€â”€ ingestion_bronze.py
â”‚ â”œâ”€â”€ dlt_pipeline.py
â”‚ â””â”€â”€ gold_dims.py
â”‚
â”œâ”€â”€ ProjectReport/ # Final report (Word document)
â”‚ â””â”€â”€ report.docx
â”‚
â””â”€â”€ README.md # Project overview


## âš™ï¸ Tech Stack  
- **Databricks Lakehouse (Serverless Compute)**  
- **Delta Lake**  
- **Autoloader** (incremental ingestion, schema evolution)  
- **Delta Live Tables (DLT)** (transformations, CDC handling, quality checks)  
- **PySpark & SQL** (star schema modeling, SCD, fact/dim tables)

## ğŸš€ Features  
- Incremental data ingestion with **Autoloader**  
- Schema inference and evolution  
- **CDC and SCD** logic for maintaining up-to-date records  
- Data quality enforcement using DLT expectations  
- Analytics-ready **Star Schema** (fact_bookings with dimension tables)  
- Scalable execution using **Serverless Compute**

